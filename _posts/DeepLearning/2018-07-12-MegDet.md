---
layout: post
title: 논문리뷰-MegDet, A Large Mini-Batch Object Detector - not yet
comments: true
category: Deep Learning
tags:
- ML
- Object Detection
- Deeplearning
- Computer Vision
- Paper review
---

![Paper_head]({{site.url}}/images/MegDet/paper_header.png){:.aligncenter}



해당 논문은 COCO 2017 Object Detection Challenge에서 1등한 논문입니다.



_주관적인 의견은 이탤릭체로 표기하였습니다._



# Summary

본 논문의 핵심은 다음과 같습니다.



* Large Mini-Batch(256)으로 Object Detection을 학습시켰더니 성능이 월등히 좋아졌습니다.
* Multi GPU를 이용해서 Batch Normalization을 계산했습니다. (Cross-GPU Batch Normalization).



![Result]({{site.url}}/images/MegDet/result.png){: width="100%", height="100%"}

<p align="center">

[MegDet]

</p>

# Problem

해당 논문은 Object Detection의 mini-batch 크기에 대해서 연구했습니다.

저자들은 small mini-batch size가 다음과 같은 문제점이 있다고 이야기합니다.



* 학습시간이 너무 길다.
* re-train시 small mini-batch size는 Batch Normalization에 정확한 통계값을 제공하지 못한다.
* small mini-batch size는 positive/negative example의 불균형을 초래한다.

## positive/negative example problem

![Result]({{site.url}}/images/MegDet/positive_negative_sample_example.png){: width="100%", height="100%"}

![Result]({{site.url}}/images/MegDet/sample_ratio_table.png){: width="100%", height="100%"}

# Challenge

* Large mini-batch size를 사용하게 됬을 때, accuracy를 유지하기 위해서 "equivalent learning rate rule"[13], [21]에 따라 더 큰 learning rate가 필요합니다.
* 큰 learning rate는 네트워크 수렴 실패의 요인이 됩니다.



[13].[Accurate, large minibatch sgd](https://arxiv.org/pdf/1706.02677)

[21].[One weird trick for parallelizing convolutional neural networks](https://arxiv.org/pdf/1404.5997)



# How to solve

* "warmup" learning rate  policy를 사용했습니다.
* Cross-GPU Batch Normalization(CGBN)을 사용해서 더 나은 Batch Normalization 통계값을 구합니다.



# Contribute

해당 논문의 컨트리뷰트는 다음과 같습니다.

* Object Detection 도메인에서 linear scaling rule에 대한 새로운 설명을 제시합니다.(loss variance가 같다는 가정)
* Cross-GPU Batch Normalization이 accuracy뿐만 아니라, 조금 더 쉽게 수렴한다는 것을 증명했습니다.
* COCO training sset에서 4시간 학습에 높은 accuracy를 보였습니다.



# Approach



## Learning Rate for Large Mini-Batch

일반적으로 Object Detection에서의 Loss 구조는 다음과 같습니다.



$$ L(x, w) = \frac{1}{N}\Sigma^{N}_{i=1}{l(x_i, w)} + \frac{\lambda}{2}||w||^{2}_{2} $$

<br/>

이는 다음과 같이 간략화 할 수 있습니다.



$$ L(x, w) = l(x, w) + l(w) $$

<br/>

$ Notation $

$ N : mini-batch &ensp; size$

$ l(x, w) :  loss$

$ l(w) : regularization&ensp; loss $

<br/>



만약 mini-batch의 크기가 $ \hat{N} \leftarrow k \cdot N$과 같이 변한다면, learning rate $ r $도 효율적인 학습을 위해서 값이 변경되어야 합니다.



"Linear Scaling Rule"[13]에서는 $\hat{r} \leftarrow k \cdot r$과 같이 learning rate를 변경합니다.

<br/>

$ Notation $

$ \hat{N} : Large &ensp; mini-batch $

$ k : accumulative &ensp; step $

$ N : small &ensp; mini-batch$

$ r : learning &ensp; rate $

<br/>



"Linear Scaling Rule"은 SGD에서  __gradient equivalence__ 가정을 기본으로 하는데, Classification 도메인에서는 증명이 되었고, 최고의 rule이지만, Object Detection에서도 이게 가능한지 확인했어야 했습니다.



저자들은 "Linear Scaling Rule"은 SGD에서  __gradient equivalence__ 가정에 대한 해석을 조금 다르게했습니다.

이럴수밖에 없었던 이유는 Classification에서는 간단한 cross-entropy loss를 사용하지만, Object Detection에서는 다양한 loss들(regression/cross-entropy 가 혼합된)을 사요여하기 때문입니다.



------------

