---
layout: post
title: 논문리뷰-MegDet, A Large Mini-Batch Object Detector
comments: true
category: Deep Learning
tags:
- ML
- Object Detection
- Deeplearning
- Computer Vision
- Paper review
---

![Paper_head]({{site.url}}/images/MegDet/paper_header.png){:.aligncenter}



<br/>

해당 논문은 COCO 2017 Object Detection Challenge에서 1등한 논문입니다.



_주관적인 의견은 이탤릭체로 표기하였습니다._

<br/>



# Summary

본 논문의 핵심은 다음과 같습니다.



* Large Mini-Batch(256)으로 Object Detection을 학습시켰더니 성능이 월등히 좋아졌습니다.



* Multi GPU를 이용해서 Batch Normalization을 계산했습니다. (Cross-GPU Batch Normalization).



![Result]({{site.url}}/images/MegDet/result.png){: width="100%", height="100%"}

<p align="center">

[MegDet]

</p>



# Contents

해당 논문 리뷰의 순서는 다음과 같습니다.

1. Problem
2. Challenge
3. How to solve
4. Contribute
5. Approach
6. Experiments

<br/>

# Problem

해당 논문은 Object Detection의 mini-batch 크기에 대해서 연구했습니다.

저자들은 small mini-batch size가 다음과 같은 문제점이 있다고 이야기합니다.



* 학습시간이 너무 길다.



* re-train시 small mini-batch size는 Batch Normalization에 정확한 통계값을 제공하지 못한다.



* small mini-batch size는 positive/negative example의 불균형을 초래한다.



<br/>

## positive/negative example problem

![Result]({{site.url}}/images/MegDet/positive_negative_sample_example.png){: width="100%", height="100%"}

![Result]({{site.url}}/images/MegDet/sample_ratio_table.png){: width="100%", height="100%"}

<br/>

# Challenge

* Large mini-batch size를 사용하게 됬을 때, accuracy를 유지하기 위해서 "equivalent learning rate rule"[13], [21]에 따라 더 큰 learning rate가 필요합니다.



* 큰 learning rate는 네트워크 수렴 실패의 요인이 됩니다.



[13].[Accurate, large minibatch sgd](https://arxiv.org/pdf/1706.02677)

[21].[One weird trick for parallelizing convolutional neural networks](https://arxiv.org/pdf/1404.5997)

<br/>

# How to solve

* "warmup" learning rate  policy를 사용했습니다.



* Cross-GPU Batch Normalization(CGBN)을 사용해서 더 나은 Batch Normalization 통계값을 구합니다.

<br/>

# Contribute

해당 논문의 컨트리뷰트는 다음과 같습니다.

* Object Detection 도메인에서 linear scaling rule에 대한 새로운 설명을 제시합니다.(loss variance가 같다는 가정)



* Cross-GPU Batch Normalization이 accuracy뿐만 아니라, 조금 더 쉽게 수렴한다는 것을 증명했습니다.



* COCO training sset에서 4시간 학습에 높은 accuracy를 보였습니다.

<br/>

# Approach



## Learning Rate for Large Mini-Batch

일반적으로 Object Detection에서의 Loss 구조는 다음과 같습니다.



$$ L(x, w) = \frac{1}{N}\Sigma^{N}_{i=1}{l(x_i, w)} + \frac{\lambda}{2}||w||^{2}_{2} $$

<br/>

이는 다음과 같이 간략화 할 수 있습니다.



$$ L(x, w) = l(x, w) + l(w) $$

<br/>

$ Notation $

$ N : mini-batch &ensp; size$

$ l(x, w) :  loss$

$ l(w) : regularization&ensp; loss $

<br/>



만약 mini-batch의 크기가 $ \hat{N} \leftarrow k \cdot N$과 같이 변한다면, learning rate $ r $도 효율적인 학습을 위해서 값이 변경되어야 합니다.



"Linear Scaling Rule"[13]에서는 $\hat{r} \leftarrow k \cdot r$과 같이 learning rate를 변경합니다.

<br/>

$ Notation $

$ \hat{N} : Large &ensp; mini-batch $

$ k : accumulative &ensp; step $

$ N : small &ensp; mini-batch$

$ r : learning &ensp; rate $

<br/>



"Linear Scaling Rule"은 SGD에서  __gradient equivalence__ 가정을 기본으로 하는데, Classification 도메인에서는 증명이 되었고, 최고의 rule이지만, Object Detection에서도 이게 가능한지 확인했어야 했습니다.



저자들은 "Linear Scaling Rule"은 SGD에서  __gradient equivalence__ 가정에 대한 해석을 조금 다르게했습니다.

이유는 Classification에서는 간단한 cross-entropy loss를 사용하지만, Object Detection에서는 다양한 loss들(regression/cross-entropy이 혼합 Loss)을 사용하기 때문입니다.

<br/>

### Variance Equivalence

_저는 이부분이 잘 이해가 안되는데, 제 아주 개인적인 생각으로는 이게 결국 $N$ 크기의 mini-batch를 돌려서 누적된 그레디언트의 분산과 $\hat{N}$ 크기의 Large mini-batch의 그레이언트의 분산이 거의 같을 것이다라는 그냥 기대값을 수식적으로 표현한 것으로 밖에 안보입니다. 결국 mini-batch size를 늘려서 Large mini-batch size를 사용한 것에 대해서 풀어낼 내용이 크지 않으니 논문에서 이렇게 말을 늘린것이 아닐까 싶습니다._

<br/>

Object Detection에서의 __gradient equivalence__ 가정은 $ k $ step 동안에 variance of gradient는 같다는 것입니다.



특정한 미니배치 크기를 갖을 때, $ l(x, w) $에 대해서 variance of gradient는 다음과 같습니다.



$$ Var(\nabla{l(x, w_t)})  = \frac{1}{N^2}\Sigma^{N}_{i=1}Var(\frac{\partial{l(x_y, w_t)}}{\partial{w_t}})$$

$$ Var(\nabla{l(x, w_t)})  = \frac{1}{N^2} \times (N \cdot \sigma^{2}_{l})$$

$$ Var(\nabla{l(x, w_t)})  = \frac{1}{N^2} \sigma^{2}_{l}$$



<br/>

$ Notation $

$ Var(x) = E((x-\mu)^2) $

$ \nabla{l(x, w_t)} $: 각 샘플에 대한 gradient

$ N $ : mini-batch 크기

$\frac{\partial{l(x_y, w_t)}}{\partial{w_t}} $ : 입력이 $ x $, weights가 $w_t $일때의 Gradient

<br/>

<br/>이와 유사하게 large mini-batch가 $\hat{N} = k \cdot N$라고 했을 때, 우리는 다음과 같이 표현할 수 있습니다.

<br/>

$$ Var(\nabla{l_{\hat{N}}(x, w_t)}) = \frac{1}{kN} \sigma^{2}_{l} $$

<br/>

여기서 우리는 weights update에서 equivalence를 기대하는 대신, Large mini-batch $\hat{N}$ 1개에서 update되는variance는 $k$번 누적되는 small mini-batch와 같길 원합니다.



이를 수식으로 표현하면 다음과 같습니다.

<br/>

$$ Var(r \cdot \Sigma^{k}_{t=1}(\nabla{l^{t}{N}(x, w)})) = r^{2} \cdot k \cdot Var(\nabla{l_{N}}(x, w)) $$

$$ Var(r \cdot \Sigma^{k}_{t=1}(\nabla{l^{t}{N}(x, w)})) \approx \hat{r}^{2} Var(\nabla{l_{\hat{N}}}(x, w)) $$



<br/>

$ Notation $

$ \hat{r}^{2} = k \cdot r $ (linear scaling rule)

$ r $: learning rate

$ k  $: small batch의 accumulative  step

<br/>



## Warmup Strategy

[13]에서 언급된 것과 같이, linear scaling rule은 weights가 불안정하게 변하는 학습 초기에는 적용하는게 불가능 할 겁니다.



그래서 MegDet에서는 [13]에서 소개된 "Linear Gradual Warmup"을 사용합니다. 

<br/>

"Linear Gradual Warmup"은

* 초기 learning rate $r$은 충분히 잡게 잡습니다.



* 몇 iteration이 끝날 때, 마다 learning rate를 점진적으로 증가시켜서 $\hat{r}$에 도달하게 합니다.

<br/>

이러한 "Linear Gradual Warmup"은 네트워크가 수렴하는데 도움은 주지만, Batch Size가 128이나 256정도로 커지면 이 방법만으로는 충분하지 않게 됩니다.

<br/>

## Cross-GPU Batch Normalization

일반적으로 Classification 네트워크들은 입력 영상의 크기가 $224 \times 224 $ or $ 229 \times 229 $ 를 사용하게 됩니다. 이런경우에는 한장의 그래픽카드에서도 batch size를 32나 그보다 더 넣을 수 있을만큼 메모리가 충분합니다. 



하지만 Object Detection에서는 그렇지 않습니다. 예를들어 FPN의 경우는 입력 영상의 크기가 $ 800 \times 800 $을 사용하고, 이렇게 입력 영상의 크기가 커진 경우에는 batch size에 한계가 생기게 됩니다.



Object Detection 시스템은 feature extractor를 Classification으로 학습하고, feature extractor를 Object Detection framework에 맞게 transfer learning을 통해서 학습하게 되는데 만약 FPN처럼 입력영상이 커져서 batch size에 제한이 걸리게 되면, Batch Normalization으로부터 구하는 sample들의 통계 파라미터들이 정확하지 않게됩니다.



MegDet은 이를 해결하고자 여러 멀티  GPU를 사용하여 학습하고, 학습시 각  Device마다 Batch Normalization 값들을 공유 후, 재계산해서 업데이트해주기 때문에 충분한 Batch Normalization의 통계 파라미터를 구할 수 있게됩니다.



Cross-GPU Batch Normalization이 어떻게 이루어지는지는와 sudo code는 아래의 그림과 같습니다.



![Result]({{site.url}}/images/MegDet/CGBN.png){: width="100%", height="100%"}

![Result]({{site.url}}/images/MegDet/CGBN_sudo_code.png){: width="100%", height="100%"}

<br/>

<br/>

# Experiments



* MegDet은 COCO Detection Dataset을 이용해서 실험했습니다.



* ImageNet에서 pre-train한 ResNet-50을 backbone network로 사용하였습니다.



* FPN을 detection framework로 사용하였습니다.



* SGD Optimizer를 사용했고 weights decay 0.0001을 주었습니다.



* learning rate는 mini-batch 크기 16을 기준으로 0.02를 주었습니다.



* linear scaling rule을 적용했습니다.

<br/>

실험은 다음과 같은 방법을 따랐습니다.



* _normal mode_  learning rate를 epoch 8과 10에서 0.1을 곱해 scaling했으며, epoch 11에 학습을 종료하였습니다.



* _long mode_ learning rate를 epoch 11과 14에서 0.1을 곱해 scaling했으며, eopch 17에는 learning rate를 절반으로 줄였습니다. 학습은 epoch 17에서 종료했습니다.

<br/>

## Large mini-batch size [no BN]



* mini batch size가 32 :  warmup을 사용했지만 학습이 실패하는 경우가 관찰되었습니다.



* mini batch size가 64 : warmup을 사용했지만 수렴에 대한 관리가 불가능했습니다. (수렴이 잘 안된다는 이야기인 것 같습니다.)



* mini batch size가 128: warmup을 사용하고 초기 learning rate를 절반으로 낮췄으나, 수렴에 대한 관리가 불가능했습니다.

<br/>

해당 실험을 통해 확인한 것은 다음과 같습니다.



1. mini-batch size가 32에서 baseline인 mini-batch 16을 기준으로 accuracy의 손실 없이 선형적인 가속을 달성했습니다.(학습 속도를 이야기하는 거 같습니다.)
2. mini-batch size가64에서 낮은 learning rate는 accuracy를 떨어뜨렸습니다.
3. mini-batch size와 learning rate를 키울수록 학습이 불가능해집니다.



아래 그림은 해당 실험 결과입니다.



![Result]({{site.url}}/images/MegDet/experiments_noBN.png){: width="100%", height="100%"}

<br/>

## Large mini-batch size, with CGBN(Cross-GPU BN)



해당 실험을 통해서 관찰된 것은 CGBN/warmup 사용 시, batch size와 상관없이 모든 학습이 잘 된다는 것입니다.



이에 대해서 간략히 요약하면 다음과 같습니다.

1. baseline(batch size 16)기준으로 batch size를 올릴 때마다, 선형적으로 성능이 좋아집니다.
2. 128개의 GPU로 COCO Dataset을 학습하는데, 4시간이면 됩니다. (baseline은 33.2시간)
3. 최적의 BN size는 32입니다. (BN size가 작으면 BN 통계값이 부정확해서 성능이 떨어지며, BN size가 너무 커고 성능 하락이 옵니다.) $ \rightarrow $ 이러한 결과는 image classification task과 object detection task간의 mismatch를 보여줍니다. (image classification task에서는 batch size를 키워주면 키워줄수록 성능이 올라가는 듯 합니다.)
4.  _long mode_ 의 학습은 조금 더 좋은 성능을 보여줍니다.



첫번째 그림은 실험결과에 대한 내용이며, 두번째 그림은 _long mode_ 에서 batch size가 16인 모델과 256인 모델의 validation 그래프를 보여줍니다. batch size 256의 _long mode_ 는 초기 epoch <16까지>에서는 baseline 보다 안좋으나 그 이후에 따라잡게 됩니다.



이러한 결과 또한 image Classification task와 Object Detection task의 차이로 보입니다.



![Result]({{site.url}}/images/MegDet/experiments_BN.png){: width="100%", height="100%"}



![Result]({{site.url}}/images/MegDet/compare_policy.png){: width="100%", height="100%"}



------------

